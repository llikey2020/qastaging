default:
  tags: ["${GIT_RUNNER_TAG}"]

variables:
  GIT_RUNNER_TAG: "k8s-cluster-1"

# Jobs that run spark extend this hidden job
.spark-run:
  image: ${SPARK_IMAGE}
  variables:
    GIT_STRATEGY: none
    SPARK_CONF_DIR: /tmp/spark/
    SPARK_IMAGE: ${CI_REGISTRY}/${SPARK_PROJECT}:${SPARK_IMAGE_TAG}
  before_script:
    # Create the Spark defaults file with the common opts
    - mkdir -p ${SPARK_CONF_DIR}
    - cd ${SPARK_CONF_DIR}
    - |
      cat << EOF > spark-defaults.conf
      spark.master                                            k8s://${KUBE_URL}
      spark.kubernetes.report.interval                        60s
      spark.driver.memory                                     ${SPARK_DRIVER_MEMORY}
      spark.executor.memory                                   ${SPARK_EXECUTOR_MEMORY}
      spark.executor.cores                                    ${SPARK_EXECUTOR_CORES}
      spark.kubernetes.executor.request.cores                 ${SPARK_EXECUTOR_REQUEST_CORES}
      spark.dynamicAllocation.enabled                         ${SPARK_DYNAMIC_ALLOCATION_ENABLED}
      spark.dynamicAllocation.shuffleTracking.enabled         ${SPARK_DYNAMIC_ALLOCATION_ENABLED}
      spark.dynamicAllocation.maxExecutors                    ${SPARK_DYNAMIC_ALLOCATION_MAX_EXECUTORS}
      spark.dynamicAllocation.minExecutors                    ${SPARK_DYNAMIC_ALLOCATION_MIN_EXECUTORS}
      spark.kubernetes.driver.pod.name                        ${SPARK_DRIVER_POD_NAME}
      spark.kubernetes.authenticate.driver.serviceAccountName spark
      spark.kubernetes.container.image                        ${SPARK_IMAGE}
      spark.kubernetes.container.image.pullSecrets            ${PULL_SECRET}
      spark.kubernetes.container.image.pullPolicy             Always
      spark.kubernetes.namespace                              ${KUBE_NAMESPACE}
      spark.jars.ivy                                          /tmp/.ivy
      spark.sql.extensions                                    io.delta.sql.DeltaSparkSessionExtension
      spark.sql.catalog.spark_catalog                         org.apache.spark.sql.delta.catalog.DeltaCatalog
      spark.delta.logStore.class                              org.apache.spark.sql.delta.storage.LocalLogStore
      spark.driver.extraJavaOptions                           -Dalluxio.master.rpc.addresses=${ALLUXIO_SVC}
      spark.executor.extraJavaOptions                         -Dalluxio.master.rpc.addresses=${ALLUXIO_SVC}
      spark.sql.warehouse.dir                                 alluxio://${ALLUXIO_SVC}/${SPARK_WAREHOUSE}
      spark.eventLog.enabled                                  ${SPARK_EVENT_LOG_ENABLED}
      spark.eventLog.dir                                      alluxio://${ALLUXIO_SVC}/${SPARK_EVENTLOG_DIR}
      # spark.metrics.* confs enable spark-dashboard to monitor Spark job
      spark.metrics.conf.*.sink.graphite.class                org.apache.spark.metrics.sink.GraphiteSink
      spark.metrics.conf.*.sink.graphite.host                 spark-dashboard-influx.${KUBE_NAMESPACE}
      spark.metrics.conf.*.sink.graphite.port                 2003
      spark.metrics.conf.*.sink.graphite.period               10
      spark.metrics.conf.*.sink.graphite.unit                 seconds
      spark.metrics.conf.*.sink.graphite.prefix               sdp
      spark.metrics.conf.*.source.jvm.class                   org.apache.spark.metrics.source.JvmSource
      spark.metrics.appStatusSource.enabled                   true
      EOF
    - cat spark-defaults.conf
