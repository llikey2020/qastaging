default:
  tags: ["kubernetes"]

workflow:
  rules:
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH

stages:
  - setup-mysql
  - setup-alluxio
  - setup-history
  - setup-hive
  - setup-metadata
  - setup-zeppelin
  - setup-frontend
  - run
  - cleanup

variables:
  DEPLOY_PROJECT:
    value: all
    description: all, alluxio, history, hive, metadata, zeppelin
  ONLY_CLEANUP: 
    value: "false"
    description: Skip cleanup when false, only run cleanup when true
  JOB:
    value: none
    description: none
  SPARK_IMAGE:
    value: gitlab.planetrover.io:5050/sequoiadp/spark:latest
    description: Spark container image to test
  SPARK_DRIVER_MEMORY:
    value: 8g
    description: spark.driver.memory
  SPARK_EXECUTOR_MEMORY:
    value: 8g
    description: spark.executor.memory
  CACHE_SSD_SIZE:
    value: 10G
    description: Cache SSD quota
  SPARK_WAREHOUSE:
    value: spark-warehouse/
    description: Path in Alluxio UFS for the Spark SQL warehouse files
  SPARK_SQL_PERF_JAR:
    value: spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar
    description: Path in Alluxio UFS for the Spark SQL performance jar
  ALLUXIO_VERSION:
    value: 2.6.0
  ALLUXIO_SVC: alluxio-master-0:19998
  SPARK_DEPENDENCY_DIR:
    value: spark-files/
    description: Path in Alluxio UFS for the Spark SQL jar(s)
  SPARK_DRIVER_POD_NAME: spark-driver
  MYSQL_VERSION: "5.7"
  MYSQL_SVC_NAME: mysql
  MYSQL_ROOT_PASSWORD: password
  METASTORE_DATABASE: metastore
  # Project variables configured in Settings -> CI/CD -> Variables:
  # - ALLUXIO_UFS
  # - AWS_ACCESS_KEY_ID
  # - AWS_SECRET_ACCESS_KEY

.environment:
  environment: testing

# Jobs that run spark extend this hidden job
.spark-run:
  image: ${SPARK_IMAGE}
  variables:
    GIT_STRATEGY: none
    SPARK_CONF_DIR: /tmp/spark/
  before_script:
    # Create the Spark defaults file with the common opts
    - mkdir -p ${SPARK_CONF_DIR}
    - cd ${SPARK_CONF_DIR}
    - |
      cat << EOF > spark-defaults.conf
      spark.master                                            k8s://${KUBE_URL}
      spark.kubernetes.report.interval                        60s
      spark.driver.memory                                     ${SPARK_DRIVER_MEMORY}
      spark.executor.memory                                   ${SPARK_EXECUTOR_MEMORY}
      spark.kubernetes.driver.pod.name                        ${SPARK_DRIVER_POD_NAME}
      spark.kubernetes.authenticate.driver.serviceAccountName spark
      spark.kubernetes.container.image                        ${SPARK_IMAGE}
      spark.kubernetes.namespace                              ${KUBE_NAMESPACE}
      spark.jars.ivy                                          /tmp/.ivy
      spark.sql.extensions                                    io.delta.sql.DeltaSparkSessionExtension
      spark.sql.catalog.spark_catalog                         org.apache.spark.sql.delta.catalog.DeltaCatalog
      spark.delta.logStore.class                              org.apache.spark.sql.delta.storage.LocalLogStore
      spark.driver.extraJavaOptions                           -Dalluxio.master.rpc.addresses=${ALLUXIO_SVC}
      spark.executor.extraJavaOptions                         -Dalluxio.master.rpc.addresses=${ALLUXIO_SVC}
      spark.sql.warehouse.dir                                 alluxio://${ALLUXIO_SVC}/${SPARK_WAREHOUSE}
      EOF
    - cat spark-defaults.conf

Setup mysql:
  image: ${CI_REGISTRY}/planetrover/infrastructure/staging
  stage: setup-mysql
  rules:
    - if: '$DEPLOY_PROJECT == "all" && $ONLY_CLEANUP == "false"'
  extends:
    - .environment
  script:
    - bash cleanup-mysql.sh || true
    - bash setup-mysql.sh
    
Setup alluxio:
  stage: setup-alluxio
  rules:
    - if: '($DEPLOY_PROJECT == "alluxio" || $DEPLOY_PROJECT == "all") && $ONLY_CLEANUP == "false"'
  trigger:
    include: services/alluxio/pipeline.yml
    strategy: depend

Setup history:
  stage: setup-history
  rules:
    - if: '($DEPLOY_PROJECT == "history" || $DEPLOY_PROJECT == "all") && $ONLY_CLEANUP == "false"'
  trigger:
    include: services/history/pipeline.yml
    strategy: depend

Setup hive:
  stage: setup-hive
  rules:
    - if: '($DEPLOY_PROJECT == "hive" || $DEPLOY_PROJECT == "all") && $ONLY_CLEANUP == "false"'
  trigger:
    include: services/hive/pipeline.yml
    strategy: depend

Setup metadata:
  stage: setup-metadata
  rules:
    - if: '($DEPLOY_PROJECT == "metadata" || $DEPLOY_PROJECT == "all") && $ONLY_CLEANUP == "false"'
  trigger:
    include: services/metadata/pipeline.yml
    strategy: depend

Setup zeppelin:
  stage: setup-zeppelin
  rules:
    - if: '($DEPLOY_PROJECT == "zeppelin" || $DEPLOY_PROJECT == "all") && $ONLY_CLEANUP == "false"'
  trigger:
    include: services/zeppelin/pipeline.yml
    strategy: depend

Setup frontend:
  stage: setup-frontend
  rules:
    - if: '($DEPLOY_PROJECT == "frontend" || $DEPLOY_PROJECT == "all") && $ONLY_CLEANUP == "false" && $JOB == "none"'
  trigger:
    include: services/frontend/pipeline.yml
    strategy: depend

Cleanup environment:
  image: ${CI_REGISTRY}/planetrover/infrastructure/staging
  rules:
    - if: '$ONLY_CLEANUP == "true"'
  stage: cleanup
  extends:
    - .environment
  script:
    - bash cleanup-env.sh
