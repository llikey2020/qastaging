variables:
  ZEPPELIN_SERVICE_NAME: zeppelin
  ZEPPELIN_PROJECT_ID: 38

Zeppelin helm prepare:
  extends:
    - .helm-prepare
  rules:
    - if: '($DEPLOY_PROJECT =~ /zeppelin/ || $DEPLOY_PROJECT == "all") && $ONLY_CLEANUP == "false"'
  variables:
    SERVICE_NAME: ${ZEPPELIN_SERVICE_NAME}
    PACKAGE_NAME: zeppelin-helm-chart
    FILE_NAME: zeppelin-0.1.0.tgz
    PACKAGE_VERSION: "0.1.0"
    ID: ${ZEPPELIN_PROJECT_ID}
    NGINX_IMAGE_TAG: "1.14.0"
    DNSMASQ_IMAGE_TAG: "release-1.0.5"
  script:
    - !reference [".helm-prepare", "script"]
    - |
      cat << EOF > ${OVERRIDES_FILE}
      imagePullSecrets:
        - name: ${PULL_SECRET}
      image:
        repository: ${REGISTRY}/sequoiadp/zeppelin:${ZEPPELIN_IMAGE_TAG}
        pullPolicy: IfNotPresent
      backendServices:
         - name: METASTORE
           value: ${SCHEMA_SERVICE_NAME}
         - name: METASTORE_NAMESPACE
           value: ${SERVICE_NAMESPACE}
         - name: SPARK_HISTORY_SERVER
           value: ${HISTORY_SERVICE_NAME}
         - name: SPARK_HISTORY_SERVER_NAMESPACE
           value: ${SERVICE_NAMESPACE}
         - name: ZEPPELIN
           value: zeppelin-server #${ZEPPELIN_SERVICE_NAME}
         - name: ZEPPELIN_NAMESPACE
           value: ${SERVICE_NAMESPACE}
      fullnameOverride: ${ZEPPELIN_SERVICE_NAME}
      serviceAccount:
        create: false
        name: ${SPARK_SERVICE_ACCOUNT}
      nginx:
        repository: nginx:${NGINX_IMAGE_TAG}
        pullPolicy: IfNotPresent
      dnsmasq:
        repository: janeczku/go-dnsmasq:${DNSMASQ_IMAGE_TAG}
        pullPolicy: IfNotPresent
      spark:
        repository: ${REGISTRY}/${SPARK_PROJECT}:${SPARK_IMAGE_TAG}
        pullPolicy: IfNotPresent
        imagePullSecret: ${PULL_SECRET}
        submitConfOptions:
          spark.kubernetes.container.image.pullPolicy: IfNotPresent
          spark.jars.ivy:                              /tmp/.ivy
          spark.hadoop.hive.metastore.uris:            thrift://${SCHEMA_SVC}
          spark.sql.sequoiadp.metaservice.uri:         ${METADATA_SVC}
          spark.sql.extensions:                        io.delta.sql.DeltaSparkSessionExtension
          spark.sql.catalog.spark_catalog:             org.apache.spark.sql.delta.catalog.DeltaCatalog
          spark.delta.logStore.class:                  org.apache.spark.sql.delta.storage.LocalLogStore
          spark.eventLog.enabled:                      true
      EOF
      if [[ ${ENABLE_ALLUXIO} -eq 1 ]]
      then
        cat << EOF >> ${OVERRIDES_FILE}
          spark.driver.extraJavaOptions:   '-Dalluxio.master.rpc.addresses=${ALLUXIO_SVC}'
          spark.executor.extraJavaOptions: '-Dalluxio.master.rpc.addresses=${ALLUXIO_SVC}'
          spark.sql.warehouse.dir:         "alluxio:///${SPARK_WAREHOUSE}"
          spark.eventLog.dir:              "alluxio:///${SPARK_EVENTLOG_DIR}"
      EOF
      else
        cat << EOF >> ${OVERRIDES_FILE}
          spark.history.provider:         org.apache.hadoop.fs.s3a.S3AFileSystem
          spark.hadoop.fs.s3a.connection.ssl.enabled: false
          spark.hadoop.fs.s3a.impl:       org.apache.hadoop.fs.s3a.S3AFileSystem
          spark.hadoop.fs.s3a.endpoint:   ${S3A_ENDPOINT}
          spark.hadoop.fs.s3a.access.key: ${S3A_ACCESS_KEY}
          spark.hadoop.fs.s3a.secret.key: ${S3A_SECRET_KEY}
          spark.sql.warehouse.dir:        "s3a://${S3A_BUCKET_NAME}/${SPARK_WAREHOUSE}"
          spark.eventLog.dir:             "s3a://${S3A_BUCKET_NAME}/${SPARK_EVENTLOG_DIR}"
          spark.history.fs.logDirectory:  "s3a://${S3A_BUCKET_NAME}/${SPARK_EVENTLOG_DIR}"
      zeppelin:
        notebookRepo:       org.apache.zeppelin.notebook.repo.S3NotebookRepo
        s3:
          endpoint:   "http://${S3A_ENDPOINT}"
          bucketName: "${S3A_BUCKET_NAME}"
          accessKey:  "${S3A_ACCESS_KEY}"
          secretKey:  "${S3A_SECRET_KEY}"
      EOF
      fi
      cat ${OVERRIDES_FILE}
