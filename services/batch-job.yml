variables:
  BATCH_JOB_SERVICE_NAME: batch-job
  BATCH_JOB_PROJECT_ID: 61

Batch job helm prepare:
  extends:
    - .helm-prepare
  rules:
    - if: '($DEPLOY_PROJECT =~ /batchJob/ || $DEPLOY_PROJECT == "all") && $ONLY_CLEANUP == "false"'
  variables:
    SERVICE_NAME: ${BATCH_JOB_SERVICE_NAME}
    PACKAGE_NAME: batch-job-helm-chart
    FILE_NAME: batch-job-0.1.0.tgz
    PACKAGE_VERSION: "0.1.0"
    ID: ${BATCH_JOB_PROJECT_ID}
    SPARK_WAREHOUSE: spark-warehouse
    SPARK_EVENTLOG_DIR: spark-logs
  script:
    - !reference [".helm-prepare", "script"]
    - |
      cat << EOF > ${OVERRIDES_FILE}
      replicaCount: ${REPLICA_COUNT}
      image:
        repository: ${REGISTRY}/sequoiadp/batch-job
        tag: ${BATCH_JOB_IMAGE_TAG}
        pullPolicy: IfNotPresent
      imagePullSecrets:
        - name: ${PULL_SECRET}
      fullnameOverride: ${BATCH_JOB_SERVICE_NAME}
      sparkJob:
        namespace: ${SERVICE_NAMESPACE}
        image: ${REGISTRY}/${SPARK_PROJECT}:${SPARK_IMAGE_TAG}
        imagePullSecrets: 
          - name: ${PULL_SECRET}
        imagePullPolicy: IfNotPresent
        sparkConf:
          spark.kubernetes.container.image.pullPolicy: IfNotPresent
          spark.jars.ivy:                              /tmp/.ivy
          spark.sql.extensions:                        io.delta.sql.DeltaSparkSessionExtension
          spark.sql.catalog.spark_catalog:             org.apache.spark.sql.delta.catalog.DeltaCatalog
          spark.delta.logStore.class:                  org.apache.spark.sql.delta.storage.LocalLogStore
          spark.eventLog.enabled:                      true
          spark.hadoop.hive.metastore.uris:            thrift://${SCHEMA_SVC}
          spark.sql.sequoiadp.metaservice.uri:         ${METADATA_SVC}
      EOF
      if [[ ${ENABLE_ALLUXIO} -eq 1 ]]
      then
        cat << EOF >> ${OVERRIDES_FILE}
          spark.sql.warehouse.dir: "alluxio://${ALLUXIO_SVC}/${SPARK_WAREHOUSE}/"
          spark.eventLog.dir:      "alluxio://${ALLUXIO_SVC}/${SPARK_EVENTLOG_DIR}/"
        driver:
          javaOptions: -Duser.timezone=${TIME_ZONE} -Dalluxio.master.rpc.addresses=${ALLUXIO_SVC}
        executor:
          javaOptions: -Duser.timezone=${TIME_ZONE} -Dalluxio.master.rpc.addresses=${ALLUXIO_SVC}
      EOF
      else
        cat << EOF >> ${OVERRIDES_FILE}
          spark.hadoop.fs.s3a.connection.ssl.enabled: false
          spark.hadoop.fs.s3a.impl:       org.apache.hadoop.fs.s3a.S3AFileSystem
          spark.hadoop.fs.s3a.endpoint:   ${S3A_ENDPOINT}
          spark.hadoop.fs.s3a.access.key: ${S3A_ACCESS_KEY}
          spark.hadoop.fs.s3a.secret.key: ${S3A_SECRET_KEY}
          spark.sql.warehouse.dir:        "s3a://${S3A_BUCKET_NAME}/${SPARK_WAREHOUSE}/"
          spark.eventLog.dir:             "s3a://${S3A_BUCKET_NAME}/${SPARK_EVENTLOG_DIR}/"
          spark.history.fs.logDirectory:  "s3a://${S3A_BUCKET_NAME}/${SPARK_EVENTLOG_DIR}/"
        driver:
          javaOptions: -Duser.timezone=${TIME_ZONE}
        executor:
          javaOptions: -Duser.timezone=${TIME_ZONE}
      sparkEventLogDir: "${SPARK_EVENTLOG_DIR}"
      s3:
        endpoint:   ${S3A_ENDPOINT}
        secret:     ${SERVICE_NAMESPACE}-s3-secret
        bucketName: ${S3A_BUCKET_NAME}
      EOF
      fi
      
      cat ${OVERRIDES_FILE}
